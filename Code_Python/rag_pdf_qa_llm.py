# -*- coding: utf-8 -*-
"""Copia_di_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QSKwYtUpAKt0VV0uwXoMkpWrfLENPfcM

# Install crucial libraries
"""

!pip install langchain-community
 !pip install langchain
 !pip install pypdf
 !pip install faiss-cpu
 !pip install langchain-chroma

!pip install bitsandbytes

"""# Imports"""

from langchain_community.document_loaders import PyPDFLoader
import pandas as pd
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
from tqdm.notebook import tqdm
import pandas as pd
import matplotlib.pyplot as plt
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores.utils import DistanceStrategy
from typing import Optional, List, Tuple
from langchain_chroma import Chroma
from transformers import pipeline
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
pd.set_option("display.max_colwidth", None)

"""
## **Part 0: Choose and Test Your Topic Without a Knowledge Base**

Before you load any external documents, you must **verify that your chosen topic needs a knowledge base** to improve answers. This ensures your RAG system solves a real gap in the model’s knowledge.

###  **Steps:**

1. **Choose a Topic (Tentative)**

   * Pick a topic from 2024 or 2025 that you think is recent or under-documented.
   * Example topics:

     * A political decision (e.g., "European Union climate laws in 2024")
     * A cultural trend (e.g., "Music trends in early 2025")

2. **Formulate Question**

   * Write down one factual, clear question about the topic.
   * Aim for question that require up-to-date or specific knowledge.

3. **Query the Model Directly**

   * Use your LLM pipeline (without RAG) to ask this question.
   * Collect the model’s answer and evaluate their quality:

     * Are the answers incomplete?
     * Are they outdated?
     * Are they confident but wrong?
     * Do they say *"I don’t know"*?

---

Why This Matters:

This step ensures your RAG project is solving a **real information gap**, not just repeating what the model already knows.
"""

#sul calcio puo essere chi vincera la scarpa doro? #magari non riconosce he è chi fa pu gol ma in base al punteggio
#oppure domande sul calcio o altro

#prova edo:
READER_MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)

READER_LLM = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    temperature=0.2,
    repetition_penalty=1.1,
    return_full_text=False,
    max_new_tokens=500,
)

READER_LLM("What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?")
#READER_LLM("How was the Tottenham Hotspur 2024–2025 Premier League ?") #general topic

"""# **Part 1: Load a Custom PDF Knowledge Base**

Find blog posts or wikipedia page with your topic and save information about it to a PDF file, and load it using `PyPDFLoader`. You may use other loaders not only pdf, but pdf loader is exactly the same as we used during lab.

- Find informative content on your topic (Wikipedia page, blog post, article, etc.)
- Save the page as a PDF file (you can use your browser’s print-to-PDF feature)
"""

# Define the path to the PDF file containing the knowledge base
file_path = "../2024–25_Tottenham_Hotspur_F.C._season.pdf"

# Create a PyPDFLoader object to handle loading of the PDF
loader = PyPDFLoader(file_path)

# Load the contents of the PDF into a variable for further processing
RAW_KNOWLEDGE_BASE = loader.load()

RAW_KNOWLEDGE_BASE[0].page_content

len(RAW_KNOWLEDGE_BASE[0].page_content)

"""
# **Part 2: Repeat the Lab with Your Own Knowledge Base + RAG Tuning**

## **Goal:**

Practice building a **RAG pipeline** and explore how **chunk size** and **chunk overlap** affect the quality of LLM answers to different questions.

---

## **What You Need to Do:**

1. **Repeat the Lab Using Your PDF Knowledge Base**

   * Use the PDF file you selected and loaded in Part 1.

2. **Create 3 Different Questions**

   * Design **three meaningful, specific questions** based on your topic.
   * Each question must be clearly related to the content of your PDF.

3. **Run RAG for Each Question with 3 Different Settings:**
   For each question:

   * Run the RAG pipeline **three times** using different settings for:

     * `chunk_size` (e.g., 100, 300, 500)
     * `chunk_overlap` (e.g., 0, 20, 50, 100)
   * This means you will run a total of **9 tests** (3 questions × 3 settings each).


4. **Answer Each Question Using an LLM**

   * Use the loaded chunks and a retriever to find relevant parts.
   * Pass the retrieved context to the LLM and generate an answer.
   * You can use similar tools as we used in the Lab

5. **Explain Your Results**
   For each of the 3 questions:

   * Write a short **description of the question** and **why you chose it**.
   * **Compare the answers** you got using different settings.
   * Reflect on:

     * How answer quality changed with different `chunk_size` and `chunk_overlap`
     * Which setting gave the most useful or accurate result
     * Why you think it performed better/worse

---

## **Deliverables:**

* Python code used for RAG pipeline (with different chunking settings)
* PDF file from Part 1
* A JSON file named rag_report_last_name_name_id.json containing your results:

  * 3 questions with explanations
  * Generated answers for each setting
  * Comparison and reflection on the results

---
"""

def plot_doc_length_distribution(document_lengths, title="Distribution of Document Lengths in the Knowledge Base"):
    """
    Plots a histogram showing the distribution of document lengths.

    Parameters:
    - document_lengths (list): A list of integers representing the number of tokens in each document.
    - title (str): Title for the plot. Defaults to a general description.
    """
    plt.figure(figsize=(10, 6))
    plt.hist(document_lengths, bins=30, color='skyblue', edgecolor='black')

    plt.title(title, fontsize=14)
    plt.xlabel("Number of Tokens", fontsize=12)
    plt.ylabel("Number of Documents", fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.show()

def split_documents(chunk_size: int, chunk_overlap: int, knowledge_base: List, tokenizer_name: str) -> List:
    """
    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.
    """
    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
        AutoTokenizer.from_pretrained(tokenizer_name),
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )

    docs_processed = text_splitter.split_documents(knowledge_base)

    return docs_processed

splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,  # The maximum number of characters in a chunk: we selected this value arbitrarily
    chunk_overlap=20,  # The number of characters to overlap between chunks
)

chunked_docs = splitter.split_documents(RAW_KNOWLEDGE_BASE)

type(chunked_docs)

len(chunked_docs)

chunked_docs[0].page_content

len(chunked_docs[0].page_content)

# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object
# max_seq_length - number of tokens
model = SentenceTransformer('thenlper/gte-small')
print(f"Model's maximum sequence length: {model.max_seq_length}")

# Load a pre-trained tokenizer from the Hugging Face model hub.
# "thenlper/gte-small" is the model identifier for a lightweight embedding model.
tokenizer = AutoTokenizer.from_pretrained("thenlper/gte-small")

# For each document chunk, tokenize the text content and calculate its token length.
# tqdm is used to display a progress bar for better visibility during processing.
lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(chunked_docs)]

plot_doc_length_distribution(document_lengths=pd.Series(lengths))

splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,  # The maximum number of characters in a chunk: we selected this value arbitrarily
    chunk_overlap=50,  # The number of characters to overlap between chunks
)

chunked_docs = splitter.split_documents(RAW_KNOWLEDGE_BASE)

lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(chunked_docs)]

plot_doc_length_distribution(document_lengths=pd.Series(lengths))

EMBEDDING_MODEL_NAME = "thenlper/gte-small"

docs_processed = split_documents(
    chunk_size=600,
    chunk_overlap=50,
    knowledge_base=RAW_KNOWLEDGE_BASE,
    tokenizer_name=EMBEDDING_MODEL_NAME,
)

# Let's visualize the chunk sizes we would have in tokens from a common model
# tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]

plot_doc_length_distribution(document_lengths=pd.Series(lengths))

embedding_model = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    multi_process=True,
    model_kwargs={"device": "cuda"},
    encode_kwargs={"normalize_embeddings": True},  # Set `True` for cosine similarity
)

vector_store = Chroma.from_documents(
    docs_processed,
    embedding_model,
    persist_directory="db5",
    collection_metadata={"hnsw:space": "cosine"}
)

results = vector_store.similarity_search_by_vector(
    embedding=embedding_model.embed_query("What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?"), k=4
)

len(results)

#comands without rag
READER_MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)

READER_LLM = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    temperature=0.2,
    repetition_penalty=1.1,
    return_full_text=False,
    max_new_tokens=500,
)

READER_LLM("What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?")

#

prompt_in_chat_format = [
    {
        "role": "system",
        "content": """Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. If the answer cannot be deduced from the context, do not give an answer.""",
    },
    {
        "role": "user",
        "content": """Context: {context}
        ---
        Now here is the question you need to answer.
        Question: {question}""",
    },
]

RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(
    prompt_in_chat_format,
    tokenize=False, # Return a string, not token IDs
    add_generation_prompt=True # Ensures model knows where to start generating
)

print(RAG_PROMPT_TEMPLATE)

retrieved_docs_text = [doc.page_content for doc in results]  # We only need the text of the documents
context = "\nExtracted documents:\n"
context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)])

len(context)

final_prompt = RAG_PROMPT_TEMPLATE.format(question="What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?", context=context)

# Redact an answer
answer = READER_LLM(final_prompt)[0]["generated_text"]

answer

#

#PRIMA DOMANDA #1o TEST: #100 size #20 overlap
docs_processed = split_documents(
    chunk_size=100,
    chunk_overlap=20,
    knowledge_base=RAW_KNOWLEDGE_BASE,
    tokenizer_name=EMBEDDING_MODEL_NAME,
)

vector_store = Chroma.from_documents(
    docs_processed,
    embedding_model,
    persist_directory="db9",
    collection_metadata={"hnsw:space": "cosine"}
)

results = vector_store.similarity_search_by_vector(
    embedding=embedding_model.embed_query("What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?"), k=4
)

retrieved_docs_text = [doc.page_content for doc in results]  # We only need the text of the documents
context = "\nExtracted documents:\n"
context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)])

final_prompt = RAG_PROMPT_TEMPLATE.format(question="What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?", context=context)

# Redact an answer
answer = READER_LLM(final_prompt)[0]["generated_text"]

answer

#PRIMA DOMANDA #2ndo TEST #300 size #50 overlap
docs_processed = split_documents(
    chunk_size=300,
    chunk_overlap=50,
    knowledge_base=RAW_KNOWLEDGE_BASE,
    tokenizer_name=EMBEDDING_MODEL_NAME,
)

vector_store = Chroma.from_documents(
    docs_processed,
    embedding_model,
    persist_directory="db9",
    collection_metadata={"hnsw:space": "cosine"}
)

results = vector_store.similarity_search_by_vector(
    embedding=embedding_model.embed_query("What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?"), k=4
)

retrieved_docs_text = [doc.page_content for doc in results]  # We only need the text of the documents
context = "\nExtracted documents:\n"
context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)])

final_prompt = RAG_PROMPT_TEMPLATE.format(question="What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?", context=context)

# Redact an answer
answer = READER_LLM(final_prompt)[0]["generated_text"]

answer

#PRIMA DOMANDA #3o TEST #500 size #100 overlap
docs_processed = split_documents(
    chunk_size=500,
    chunk_overlap=100,
    knowledge_base=RAW_KNOWLEDGE_BASE,
    tokenizer_name=EMBEDDING_MODEL_NAME,
)

vector_store = Chroma.from_documents(
    docs_processed,
    embedding_model,
    persist_directory="db9",
    collection_metadata={"hnsw:space": "cosine"}
)

results = vector_store.similarity_search_by_vector(
    embedding=embedding_model.embed_query("What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?"), k=4
)

retrieved_docs_text = [doc.page_content for doc in results]  # We only need the text of the documents
context = "\nExtracted documents:\n"
context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)])

final_prompt = RAG_PROMPT_TEMPLATE.format(question="What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?", context=context)

# Redact an answer
answer = READER_LLM(final_prompt)[0]["generated_text"]

answer

##############

#SECONDA DOMANDA #1o TEST #100 size #20 overlap
docs_processed = split_documents(
    chunk_size=100,
    chunk_overlap=20,
    knowledge_base=RAW_KNOWLEDGE_BASE,
    tokenizer_name=EMBEDDING_MODEL_NAME,
)

vector_store = Chroma.from_documents(
    docs_processed,
    embedding_model,
    persist_directory="db9",
    collection_metadata={"hnsw:space": "cosine"}
)

results = vector_store.similarity_search_by_vector(
    embedding=embedding_model.embed_query("What position did Tottenham Hotspur finish in the 2024–25 Premier League, and how was their domestic performance compared to their European success?"), k=4
)

retrieved_docs_text = [doc.page_content for doc in results]  # We only need the text of the documents
context = "\nExtracted documents:\n"
context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)])

final_prompt = RAG_PROMPT_TEMPLATE.format(question="What position did Tottenham Hotspur finish in the 2024–25 Premier League, and how was their domestic performance compared to their European success?", context=context)

# Redact an answer
answer = READER_LLM(final_prompt)[0]["generated_text"]

answer

#SECONDA DOMANDA #2ndo TEST #300 size #50 overlap
docs_processed = split_documents(
    chunk_size=300,
    chunk_overlap=50,
    knowledge_base=RAW_KNOWLEDGE_BASE,
    tokenizer_name=EMBEDDING_MODEL_NAME,
)

vector_store = Chroma.from_documents(
    docs_processed,
    embedding_model,
    persist_directory="db9",
    collection_metadata={"hnsw:space": "cosine"}
)

results = vector_store.similarity_search_by_vector(
    embedding=embedding_model.embed_query("What position did Tottenham Hotspur finish in the 2024–25 Premier League, and how was their domestic performance compared to their European success?"), k=4
)

retrieved_docs_text = [doc.page_content for doc in results]  # We only need the text of the documents
context = "\nExtracted documents:\n"
context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)])

final_prompt = RAG_PROMPT_TEMPLATE.format(question="What position did Tottenham Hotspur finish in the 2024–25 Premier League, and how was their domestic performance compared to their European success?", context=context)

# Redact an answer
answer = READER_LLM(final_prompt)[0]["generated_text"]

answer

#SECONDA DOMANDA #3o TEST #500 size #100 overlap
docs_processed = split_documents(
    chunk_size=500,
    chunk_overlap=100,
    knowledge_base=RAW_KNOWLEDGE_BASE,
    tokenizer_name=EMBEDDING_MODEL_NAME,
)

vector_store = Chroma.from_documents(
    docs_processed,
    embedding_model,
    persist_directory="db9",
    collection_metadata={"hnsw:space": "cosine"}
)

results = vector_store.similarity_search_by_vector(
    embedding=embedding_model.embed_query("What position did Tottenham Hotspur finish in the 2024–25 Premier League, and how was their domestic performance compared to their European success?"), k=4
)

retrieved_docs_text = [doc.page_content for doc in results]  # We only need the text of the documents
context = "\nExtracted documents:\n"
context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)])

final_prompt = RAG_PROMPT_TEMPLATE.format(question="What position did Tottenham Hotspur finish in the 2024–25 Premier League, and how was their domestic performance compared to their European success?", context=context)

# Redact an answer
answer = READER_LLM(final_prompt)[0]["generated_text"]

answer

################

#TERZA DOMANDA #1o TEST #100 size #20 overlap
docs_processed = split_documents(
    chunk_size=100,
    chunk_overlap=20,
    knowledge_base=RAW_KNOWLEDGE_BASE,
    tokenizer_name=EMBEDDING_MODEL_NAME,
)

vector_store = Chroma.from_documents(
    docs_processed,
    embedding_model,
    persist_directory="db9",
    collection_metadata={"hnsw:space": "cosine"}
)

results = vector_store.similarity_search_by_vector(
    embedding=embedding_model.embed_query("Did Tottenham Hotspur win Europa League?"), k=4
)

retrieved_docs_text = [doc.page_content for doc in results]  # We only need the text of the documents
context = "\nExtracted documents:\n"
context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)])

final_prompt = RAG_PROMPT_TEMPLATE.format(question="Did Tottenham Hotspur win Europa League?", context=context)

# Redact an answer
answer = READER_LLM(final_prompt)[0]["generated_text"]

answer

#
#TERZA DOMANDA #2o TEST #300 size #50 overlap
docs_processed = split_documents(
    chunk_size=300,
    chunk_overlap=50,
    knowledge_base=RAW_KNOWLEDGE_BASE,
    tokenizer_name=EMBEDDING_MODEL_NAME,
)

vector_store = Chroma.from_documents(
    docs_processed,
    embedding_model,
    persist_directory="db9",
    collection_metadata={"hnsw:space": "cosine"}
)

results = vector_store.similarity_search_by_vector(
    embedding=embedding_model.embed_query("Did Tottenham Hotspur win Europa League?"), k=4
)

retrieved_docs_text = [doc.page_content for doc in results]  # We only need the text of the documents
context = "\nExtracted documents:\n"
context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)])

final_prompt = RAG_PROMPT_TEMPLATE.format(question="Did Tottenham Hotspur win Europa League?", context=context)

# Redact an answer
answer = READER_LLM(final_prompt)[0]["generated_text"]

answer

#
#TERZA DOMANDA #3o TEST #500 size #100 overlap
docs_processed = split_documents(
    chunk_size=500,
    chunk_overlap=100,
    knowledge_base=RAW_KNOWLEDGE_BASE,
    tokenizer_name=EMBEDDING_MODEL_NAME,
)

vector_store = Chroma.from_documents(
    docs_processed,
    embedding_model,
    persist_directory="db9",
    collection_metadata={"hnsw:space": "cosine"}
)

results = vector_store.similarity_search_by_vector(
    embedding=embedding_model.embed_query("Did Tottenham Hotspur win Europa League?"), k=4
)

retrieved_docs_text = [doc.page_content for doc in results]  # We only need the text of the documents
context = "\nExtracted documents:\n"
context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(retrieved_docs_text)])

final_prompt = RAG_PROMPT_TEMPLATE.format(question="Did Tottenham Hotspur win Europa League?", context=context)

# Redact an answer
answer = READER_LLM(final_prompt)[0]["generated_text"]

answer

##

"""### Template for your resulting json file with report"""

your_results_dict = {
  "topic": "Tottenham Hotspur 2024–25 season",
  "question":"What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?",
  "answer":"[{'generated_text': '\n   A) First place, Champions League qualification\n   B) Second place, Europa Conference League qualification\n   C) Third place, UEFA Cup/Europa League qualification\n   D) Fourth place, domestic cup finalist status\n   \n   Answer: The correct answer is not provided directly within this text. However, based on common knowledge and assuming no extraordinary circumstances affected their usual performance (as per your request to avoid using 'didn\'t', which would imply a negative outcome), it can be inferred that they likely finished either first or second as these are typical outcomes for top-performing teams like Tottenham Hotspur. For an accurate response regarding specific league rankings such as those of football clubs, one should refer to official sports news sources from the relevant season.'}]",
  "rag": [
    {
      "question": "What position did Tottenham Hotspur finish in the 2024–2025 Premier League standings?",
      "reason": "This question is essential to assess the club's domestic league performance and identify any gap between expectation and outcome, which can inform evaluations of management and strategy.",
      "experiments": [
        {
          "chunk_size": 100,
          "chunk_overlap": 20,
          "answer": "Tottenham Hotspur finished fifth in the 2024–2025 Premier League standings.",
          "reflection": "This answer is inaccurate. The model pulled a result from the previous season (2023–24) rather than the current one. It suggests that with small chunks and minimal overlap, key context (such as the final league table) might be missed or incorrectly linked."
        },
        {
          "chunk_size": 300,
          "chunk_overlap": 50,
          "answer": "Tottenham Hotspur finished fifth in the 2024–2025 Premier League standings.",
          "reflection": "Despite a larger chunk size and increased overlap, the model still retrieved the incorrect position. This confirms the importance of precise table extraction or the limitations of the retriever with outdated or misaligned context."
        },
        {
          "chunk_size": 500,
          "chunk_overlap": 100,
          "answer": "Tottenham Hotspur finished 17th in the 2024–2025 Premier League standings.",
          "reflection": "This answer is accurate and matches the PDF source. The larger chunk size and high overlap likely allowed the retriever to access full standings context, showing improved performance with broader context windows."
        }
      ]
    },
    {
      "question": "What position did Tottenham Hotspur finish in the 2024–25 Premier League, and how was their domestic performance compared to their European success?",
      "reason": "This question evaluates the disparity between domestic and European results, helping to understand inconsistencies in club performance across competitions.",
      "experiments": [
        {
          "chunk_size": 100,
          "chunk_overlap": 20,
          "answer": "Tottenham Hotspur finished 17th in the 2024–25 Premier League, which represented their worst top-flight finish in 48 years since the 1976–77 season. Their domestic performance this season can be described as disappointing when considering they won the UEFA Europa League but ended up last among England’s elite clubs due to accumulating fewer than half the total number of points available during that period. Despite these struggles domestically, it highlights significant disparity given their successful campaign on the continental stage wherein securing back-to-back titles after nearly two decades underscores exceptional achievement against expectations based solely upon prior performances within the national leagues.",
          "reflection": "A solid, relevant answer. It identifies the contrast clearly, though the language is slightly verbose. With small chunks, the retrieval worked because the info was likely contained together in a concise paragraph."
        },
        {
          "chunk_size": 300,
          "chunk_overlap": 50,
          "answer": "Tottenham Hotspur finished 17th in the 2024–25 Premier League. Their domestic performance was poor as they had their worst ever top-flight finish in terms of ranking; however, it paled when juxtaposed against their significant European triumph wherein they clinched their first major trophy in 17 years by winning the UEFA Europa League on penalt0als after securing a crucial last-minute goal during extra time which ensured them qualifying for the following year’s UEFA Champions League tournament. Despite this accomplishment, domestically, they ended up being close behind teams like Manchester United who won both leagues that same season but still managed better overall results than Tottenham due to more consistent performances throughout the campaign.",
          "reflection": "This answer is mostly accurate but includes a factual error (the final was 1–0, not on penalties). It suggests that with a wider context, there may be risk of noise or mixed facts unless documents are cleanly segmented."
        },
        {
          "chunk_size": 500,
          "chunk_overlap": 100,
          "answer": "Tottenham Hotspur finished 17th in the 2024–25 Premier League. Their domestic performance was poor as they had their worst ever top-flight finish in terms of points; however, this low ranking allowed them to avoid immediate relegation by securing exactly enough points to stay ahead of three clubs that were demoted - Leicester City, Ipswich Town, and Southampton. Despite these struggles domestically, their European campaign saw significant success, culminating in winning the UEFA Europa League after defeating Manchester United on penalties in the final.",
          "reflection": "This is the most accurate and comprehensive answer, but includes again the same factual error (the final was 1–0, not on penalties). The full narrative and detail from the source are preserved, demonstrating how larger chunks and overlap yield better summaries of complex scenarios, but there could be some mistakes too."
        }
      ]
    },
    {
      "question": "Did Tottenham Hotspur win Europa League?",
      "reason": "This question checks for retrieval of a major event in the club’s season, and whether the model correctly integrates key success outcomes.",
      "experiments": [
        {
          "chunk_size": 100,
          "chunk_overlap": 20,
          "answer": "Yes, on 21 May 2025, Tottenham Hotspur won the UEFA Europa League by securing a 1–0 victory over Manchester United in the final match.",
          "reflection": "This answer is accurate and concise. Even at small chunk sizes, key events like a final win are retrieved effectively if mentioned prominently in the source."
        },
        {
          "chunk_size": 300,
          "chunk_overlap": 50,
          "answer": "Yes, Tottenham Hotspur won the UEFA Europa League on 21 May 2025 by securing a 1–0 victory against Manchester United.",
          "reflection": "Same as previous, showing consistent retrieval for clear, central facts. Chunk size did not materially change the output quality for this simple question."
        },
        {
          "chunk_size": 500,
          "chunk_overlap": 100,
          "answer": "Yes, Tottenham Hotspur won the UEFA Europa League on 21 May 2025 by securing a 1–0 victory against Manchester United.",
          "reflection": "Identical to previous, reinforcing that for straightforward questions with clearly stated answers, even small chunks are effective."
        }
      ]
    }
  ]
}

import json

with open("rag_report_Edoardo_Lovato.json", "w", encoding="utf-8") as f:
    json.dump(your_results_dict, f, indent=2, ensure_ascii=False)